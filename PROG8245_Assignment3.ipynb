{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Sreehari Prathap\n",
    "- ID: 8903199\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'is', 'a', 'versatile', 'programming', 'language', ',', 'python', 'proved', 'it', 'importance', 'in', 'various', 'domain', '.'], ['javascript', 'is', 'widely', 'used', 'for', 'web', 'development', '.'], ['java', 'is', 'known', 'for', 'it', 'platform', 'independence', '.'], ['programming', 'involves', 'writing', 'code', 'to', 'solve', 'problem', '.'], ['data', 'structure', 'are', 'crucial', 'for', 'efficient', 'programming', '.'], ['algorithm', 'are', 'step', '-', 'by', '-', 'step', 'instruction', 'for', 'solving', 'problem', '.'], ['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'in', 'collaboration', '.'], ['debugging', 'is', 'the', 'process', 'of', 'finding', 'and', 'fixing', 'error', 'in', 'python', 'code', '.'], ['web', 'framework', 'simplify', 'the', 'development', 'of', 'web', 'application', '.'], ['artificial', 'intelligence', 'can', 'be', 'applied', 'in', 'various', 'programming', 'task', '.']]\n"
     ]
    }
   ],
   "source": [
    "# ## You are allowed for PART A to use any library that would help you in the task.\n",
    "# def clean_sentences(sentences=None):\n",
    "#     ## This function takes as an input list of sentences\n",
    "#     ## This function returns a list of tokenized_sentences\n",
    "#     return [[\"token1\",\"token2\"], [\"token3\", \"token4\"]]\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load SpaCy English model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_sentences(sentences=None, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess a list of sentences to tokenize, normalize, stem/lemmatize, and handle NER.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): List of input sentences.\n",
    "        lemmatize (bool): Whether to use lemmatization (True) or stemming (False).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tokenized sentences with preprocessing applied.\n",
    "    \"\"\"\n",
    "    # Initialize stemmer or lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Step 1: Perform Named Entity Recognition using SpaCy\n",
    "        doc = nlp(sentence)\n",
    "        ner_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.ent_type_:  # If part of a named entity, add as a whole (case-folded)\n",
    "                ner_tokens.append(token.text.lower())\n",
    "            else:  # Otherwise, process token normally\n",
    "                ner_tokens.append(token.text)\n",
    "        \n",
    "        # Step 2: Tokenization using NLTK\n",
    "        tokens = word_tokenize(\" \".join(ner_tokens))  # Tokenize after ensuring named entities are handled\n",
    "        \n",
    "        # Step 3: Normalization - Lowercasing tokens (excluding named entities already processed)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        # Step 4: Stemming or Lemmatization\n",
    "        if lemmatize:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        else:\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # Append processed tokens to the list\n",
    "        processed_sentences.append(tokens)\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "sentences = clean_sentences(sample_sentences)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "    for sentence_id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(sentence_id)\n",
    "    return inverted_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    output = {\"token\":{1:[0],\n",
    "                     2:[3,15]}\n",
    "                     } \n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': [(1, 0), (2, 0)], 'is': [(1, 1), (2, 1), (3, 2)], 'a': [(1, 2)], 'sample': [(1, 3), (3, 0), (3, 3)], 'another': [(2, 2)], 'example': [(2, 3)], 'document': [(3, 1)]}\n"
     ]
    }
   ],
   "source": [
    "def create_positional_index(sentence_tokens):\n",
    "    \"\"\"\n",
    "    Creates a positional index for a 2-dimensional list of sentence tokens.\n",
    "\n",
    "    :param sentence_tokens: List of sentences where each sentence is a list of tokens\n",
    "    :return: Dictionary representing the positional index\n",
    "    \"\"\"\n",
    "    positional_index = {}\n",
    "\n",
    "    # Iterate through each sentence and its tokens\n",
    "    for sentence_id, tokens in enumerate(sentence_tokens, start=1):\n",
    "        for position, token in enumerate(tokens):\n",
    "            # Initialize entry if token not already in the index\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = []\n",
    "            \n",
    "            # Append the sentence ID and position of the token\n",
    "            positional_index[token].append((sentence_id, position))\n",
    "    \n",
    "    return positional_index\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    [\"this\", \"is\", \"a\", \"sample\"],\n",
    "    [\"this\", \"is\", \"another\", \"example\"],\n",
    "    [\"sample\", \"document\", \"is\", \"sample\"]\n",
    "]\n",
    "\n",
    "positional_index = create_positional_index(sentences)\n",
    "print(positional_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # Prepare the ranking list\n",
    "    rank_list = []\n",
    "\n",
    "    # Flatten the list of sentences into a document corpus\n",
    "    # and get a unique list of all tokens (vocabulary)\n",
    "    all_tokens = [token for sentence in list_of_sentence_tokens for token in sentence]\n",
    "    vocabulary = set(all_tokens)\n",
    "\n",
    "    # Total number of documents (sentences)\n",
    "    num_documents = len(list_of_sentence_tokens)\n",
    "\n",
    "    # Function to compute Term Frequency (TF)\n",
    "    def compute_tf(sentence_tokens, token):\n",
    "        return sentence_tokens.count(token) / len(sentence_tokens)\n",
    "\n",
    "    # Function to compute Inverse Document Frequency (IDF)\n",
    "    def compute_idf(token):\n",
    "        containing_documents = sum(1 for sentence in list_of_sentence_tokens if token in sentence)\n",
    "        return log(num_documents / (1 + containing_documents))  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Logic for \"inverted\" method\n",
    "    if method_name == \"inverted\":\n",
    "        query_tokens = search_query.split()  # Split query into individual tokens\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            # Count the number of matching query tokens in the sentence\n",
    "            match_count = sum(1 for token in query_tokens if token in sentence)\n",
    "            rank_list.append((i, match_count))  # Add sentence index and match count\n",
    "\n",
    "        # Sort by match count (descending) and return sentence indices\n",
    "        rank_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        rank_list = [doc[0] for doc in rank_list]\n",
    "\n",
    "    # Logic for \"tfidf\" method\n",
    "    elif method_name == \"tfidf\":\n",
    "        query_tokens = search_query.split()\n",
    "        sentence_scores = []\n",
    "\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            tfidf_score = 0\n",
    "            for token in query_tokens:\n",
    "                if token in vocabulary:\n",
    "                    tf = compute_tf(sentence, token)\n",
    "                    idf = compute_idf(token)\n",
    "                    tfidf_score += tf * idf\n",
    "            sentence_scores.append((i, tfidf_score))  # Add sentence index and score\n",
    "\n",
    "        # Sort by tf-idf score (descending) and return sentence indices\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        rank_list = [doc[0] for doc in sentence_scores]\n",
    "\n",
    "    return rank_list\n",
    "\n",
    "# Example Usage:\n",
    "sentences = [\n",
    "    [\"this\", \"is\", \"a\", \"sample\"],\n",
    "    [\"this\", \"is\", \"another\", \"example\"],\n",
    "    [\"sample\", \"document\", \"is\", \"sample\"]\n",
    "]\n",
    "query = \"sample example\"\n",
    "print(get_ranked_documents(sentences, \"inverted\", query))  # Ranking using inverted index\n",
    "print(get_ranked_documents(sentences, \"tfidf\", query))    # Ranking using tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0, 0, 0, -0.0, -0.0, -0.0, -0.0]\n",
      "[0, 0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "[-0.0, 0.0, -0.0, 0, 0, 0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_mtf(term, doc, term_positions):\n",
    "    \"\"\"\n",
    "    Calculate the Modified Term Frequency (MTF) for a term in a document.\n",
    "    :param term: The term for which to compute MTF.\n",
    "    :param doc: The document in which the term occurs.\n",
    "    :param term_positions: Dictionary of terms and their positions in the document.\n",
    "    :return: MTF value for the term in the document.\n",
    "    \"\"\"\n",
    "    freq = doc.count(term)  # Raw frequency of the term in the document\n",
    "    position = term_positions.get(term, -1)  # Position of the first occurrence of the term\n",
    "    if position == -1:\n",
    "        return 0  # If the term doesn't exist, return 0\n",
    "    return freq / (1 + position)\n",
    "\n",
    "def compute_midf(term, document_lengths, document_frequency, total_documents, M=5):\n",
    "    \"\"\"\n",
    "    Calculate the Modified Inverse Document Frequency (MIDF) for a term.\n",
    "    :param term: The term for which to compute MIDF.\n",
    "    :param document_lengths: A dictionary mapping document IDs to their lengths.\n",
    "    :param document_frequency: The number of documents containing the term.\n",
    "    :param total_documents: The total number of documents in the dataset.\n",
    "    :param M: The scaling factor, default is 5.\n",
    "    :return: MIDF value for the term.\n",
    "    \"\"\"\n",
    "    sum_doc_lengths = sum(document_lengths[d] for d in document_frequency[term])  # Sum of lengths of docs containing the term\n",
    "    midf = math.log(total_documents / (len(document_frequency[term]) * (1 / M) * sum_doc_lengths))\n",
    "    return midf\n",
    "\n",
    "def get_modified_tfidf_matrix(list_of_sentence_tokens, M=5):\n",
    "    \"\"\"\n",
    "    Compute the Modified TF-IDF matrix for a list of tokenized sentences.\n",
    "    :param list_of_sentence_tokens: List of sentences, where each sentence is a list of tokens (terms).\n",
    "    :param M: The scaling factor for MIDF calculation.\n",
    "    :return: A matrix of Modified TF-IDF values.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess to gather term positions, document lengths, and document frequency\n",
    "    term_positions = defaultdict(dict)\n",
    "    document_frequency = defaultdict(set)\n",
    "    document_lengths = defaultdict(int)\n",
    "    total_documents = len(list_of_sentence_tokens)\n",
    "    \n",
    "    # Collect information about term frequencies, positions, and document lengths\n",
    "    for doc_id, doc in enumerate(list_of_sentence_tokens):\n",
    "        document_lengths[doc_id] = len(doc)\n",
    "        for idx, term in enumerate(doc):\n",
    "            if term not in term_positions[term]:\n",
    "                term_positions[term][doc_id] = idx  # Record the first position of the term in the document\n",
    "            document_frequency[term].add(doc_id)  # Record that the term appears in this document\n",
    "    \n",
    "    # Step 2: Calculate MTF and MIDF for each term in each document and fill the matrix\n",
    "    term_set = sorted(set(term for doc in list_of_sentence_tokens for term in doc))  # List of unique terms across all docs\n",
    "    term_index = {term: idx for idx, term in enumerate(term_set)}  # Mapping terms to index\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for doc_id, doc in enumerate(list_of_sentence_tokens):\n",
    "        row = [0] * len(term_set)  # Initialize a row for the document with zeros\n",
    "        for term in set(doc):  # Iterate over unique terms in the document\n",
    "            # Calculate MTF and MIDF for the term in the document\n",
    "            mtf_value = compute_mtf(term, doc, term_positions[term])\n",
    "            midf_value = compute_midf(term, document_lengths, document_frequency, total_documents, M)\n",
    "            # Calculate the final weight as MTF * MIDF\n",
    "            weight = mtf_value * midf_value\n",
    "            row[term_index[term]] = weight  # Assign the weight to the corresponding index in the row\n",
    "        tfidf_matrix.append(row)\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"cat\", \"chased\", \"the\", \"dog\"]\n",
    "]\n",
    "\n",
    "matrix = get_modified_tfidf_matrix(documents, M=5)\n",
    "for row in matrix:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with M = 1\n",
      "[-0.0, 0, 0, -0.0, -0.0, -0.0, -0.0]\n",
      "[0, 0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "[-0.0, -0.0, -0.0, 0, 0, 0, -0.0]\n",
      "\n",
      "\n",
      "Results with M = 5\n",
      "[-0.0, 0, 0, -0.0, -0.0, -0.0, -0.0]\n",
      "[0, 0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "[-0.0, 0.0, -0.0, 0, 0, 0, -0.0]\n",
      "\n",
      "\n",
      "Results with M = 10\n",
      "[0.0, 0, 0, 0.0, 0.0, 0.0, -0.0]\n",
      "[0, 0, 0.0, 0.0, 0.0, 0.0, -0.0]\n",
      "[0.0, 0.0, 0.0, 0, 0, 0, -0.0]\n",
      "\n",
      "\n",
      "Results with M = 50\n",
      "[0.0, 0, 0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0, 0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0, 0, 0, 0.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with different values of M\n",
    "documents = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"cat\", \"chased\", \"the\", \"dog\"]\n",
    "]\n",
    "\n",
    "# Try different M values\n",
    "for M_value in [1, 5, 10, 50]:\n",
    "    print(f\"Results with M = {M_value}\")\n",
    "    matrix = get_modified_tfidf_matrix(documents, M=M_value)\n",
    "    for row in matrix:\n",
    "        print(row)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling factor \n",
    "ùëÄ\n",
    "in the MIDF calculation controls how much the document length affects the term weights:\n",
    "\n",
    "- Small \n",
    "ùëÄ values (like 1 or 5): Document length has a big impact, making longer documents more significant.\n",
    "- Larger \n",
    "ùëÄ values (like 10 or 50): Document length matters less, and the focus shifts to how rare a term is across documents.\n",
    "\n",
    "So, \n",
    "ùëÄ adjusts the balance between the document's length and the term's rarity, with higher values giving more importance to the term's rarity and less to document length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Modified TF-Modified IDF technique can be useful, but it has its pros and cons. Here's a simple breakdown:\n",
    "\n",
    "**Pros**:\n",
    "Takes Document Length Into Account: By adjusting for document length, it ensures that longer documents don't unfairly boost term weights. This is useful when dealing with datasets where documents vary in size.\n",
    "Considers Term Position: Giving higher weight to terms that appear earlier in the document can capture important keywords that influence the document's topic or meaning.\n",
    "\n",
    "**Cons**:\n",
    "Complexity: It adds complexity compared to standard TF-IDF. This could be unnecessary if document length or term position isn‚Äôt a key factor for your analysis.\n",
    "Less Effective for Short Texts: For short documents (like tweets or short reviews), this method might not add significant value and could overemphasize certain words or positions.\n",
    "\n",
    "**Conclusion**:\n",
    "It‚Äôs a good technique if you're working with documents of varying lengths and want to prioritize terms that appear early. However, it might be overkill in simpler or more uniform datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "npl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
