{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Sreehari Prathap\n",
    "- ID: 8903199\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sreeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'is', 'a', 'versatile', 'programming', 'language', ',', 'python', 'proved', 'it', 'importance', 'in', 'various', 'domain', '.'], ['javascript', 'is', 'widely', 'used', 'for', 'web', 'development', '.'], ['java', 'is', 'known', 'for', 'it', 'platform', 'independence', '.'], ['programming', 'involves', 'writing', 'code', 'to', 'solve', 'problem', '.'], ['data', 'structure', 'are', 'crucial', 'for', 'efficient', 'programming', '.'], ['algorithm', 'are', 'step', '-', 'by', '-', 'step', 'instruction', 'for', 'solving', 'problem', '.'], ['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'in', 'collaboration', '.'], ['debugging', 'is', 'the', 'process', 'of', 'finding', 'and', 'fixing', 'error', 'in', 'python', 'code', '.'], ['web', 'framework', 'simplify', 'the', 'development', 'of', 'web', 'application', '.'], ['artificial', 'intelligence', 'can', 'be', 'applied', 'in', 'various', 'programming', 'task', '.']]\n"
     ]
    }
   ],
   "source": [
    "# ## You are allowed for PART A to use any library that would help you in the task.\n",
    "# def clean_sentences(sentences=None):\n",
    "#     ## This function takes as an input list of sentences\n",
    "#     ## This function returns a list of tokenized_sentences\n",
    "#     return [[\"token1\",\"token2\"], [\"token3\", \"token4\"]]\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load SpaCy English model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_sentences(sentences=None, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess a list of sentences to tokenize, normalize, stem/lemmatize, and handle NER.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): List of input sentences.\n",
    "        lemmatize (bool): Whether to use lemmatization (True) or stemming (False).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tokenized sentences with preprocessing applied.\n",
    "    \"\"\"\n",
    "    # Initialize stemmer or lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Step 1: Perform Named Entity Recognition using SpaCy\n",
    "        doc = nlp(sentence)\n",
    "        ner_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.ent_type_:  # If part of a named entity, add as a whole (case-folded)\n",
    "                ner_tokens.append(token.text.lower())\n",
    "            else:  # Otherwise, process token normally\n",
    "                ner_tokens.append(token.text)\n",
    "        \n",
    "        # Step 2: Tokenization using NLTK\n",
    "        tokens = word_tokenize(\" \".join(ner_tokens))  # Tokenize after ensuring named entities are handled\n",
    "        \n",
    "        # Step 3: Normalization - Lowercasing tokens (excluding named entities already processed)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        # Step 4: Stemming or Lemmatization\n",
    "        if lemmatize:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        else:\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # Append processed tokens to the list\n",
    "        processed_sentences.append(tokens)\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "sentences = clean_sentences(sample_sentences)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': [1, 2], 'is': [1, 2], 'a': [1, 2], 'test': [1, 2, 2, 3], 'another': [3]}\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "    for sentence_id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(sentence_id)\n",
    "    return inverted_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    output = {\"token\":{1:[0],\n",
    "                     2:[3,15]}\n",
    "                     } \n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': [(1, 0), (2, 0)], 'is': [(1, 1), (2, 1), (3, 2)], 'a': [(1, 2)], 'sample': [(1, 3), (3, 0), (3, 3)], 'another': [(2, 2)], 'example': [(2, 3)], 'document': [(3, 1)]}\n"
     ]
    }
   ],
   "source": [
    "def create_positional_index(sentence_tokens):\n",
    "    \"\"\"\n",
    "    Creates a positional index for a 2-dimensional list of sentence tokens.\n",
    "\n",
    "    :param sentence_tokens: List of sentences where each sentence is a list of tokens\n",
    "    :return: Dictionary representing the positional index\n",
    "    \"\"\"\n",
    "    positional_index = {}\n",
    "\n",
    "    # Iterate through each sentence and its tokens\n",
    "    for sentence_id, tokens in enumerate(sentence_tokens, start=1):\n",
    "        for position, token in enumerate(tokens):\n",
    "            # Initialize entry if token not already in the index\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = []\n",
    "            \n",
    "            # Append the sentence ID and position of the token\n",
    "            positional_index[token].append((sentence_id, position))\n",
    "    \n",
    "    return positional_index\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    [\"this\", \"is\", \"a\", \"sample\"],\n",
    "    [\"this\", \"is\", \"another\", \"example\"],\n",
    "    [\"sample\", \"document\", \"is\", \"sample\"]\n",
    "]\n",
    "\n",
    "positional_index = create_positional_index(sentences)\n",
    "print(positional_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # Prepare the ranking list\n",
    "    rank_list = []\n",
    "\n",
    "    # Flatten the list of sentences into a document corpus\n",
    "    # and get a unique list of all tokens (vocabulary)\n",
    "    all_tokens = [token for sentence in list_of_sentence_tokens for token in sentence]\n",
    "    vocabulary = set(all_tokens)\n",
    "\n",
    "    # Total number of documents (sentences)\n",
    "    num_documents = len(list_of_sentence_tokens)\n",
    "\n",
    "    # Function to compute Term Frequency (TF)\n",
    "    def compute_tf(sentence_tokens, token):\n",
    "        return sentence_tokens.count(token) / len(sentence_tokens)\n",
    "\n",
    "    # Function to compute Inverse Document Frequency (IDF)\n",
    "    def compute_idf(token):\n",
    "        containing_documents = sum(1 for sentence in list_of_sentence_tokens if token in sentence)\n",
    "        return log(num_documents / (1 + containing_documents))  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Logic for \"inverted\" method\n",
    "    if method_name == \"inverted\":\n",
    "        query_tokens = search_query.split()  # Split query into individual tokens\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            # Count the number of matching query tokens in the sentence\n",
    "            match_count = sum(1 for token in query_tokens if token in sentence)\n",
    "            rank_list.append((i, match_count))  # Add sentence index and match count\n",
    "\n",
    "        # Sort by match count (descending) and return sentence indices\n",
    "        rank_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        rank_list = [doc[0] for doc in rank_list]\n",
    "\n",
    "    # Logic for \"tfidf\" method\n",
    "    elif method_name == \"tfidf\":\n",
    "        query_tokens = search_query.split()\n",
    "        sentence_scores = []\n",
    "\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            tfidf_score = 0\n",
    "            for token in query_tokens:\n",
    "                if token in vocabulary:\n",
    "                    tf = compute_tf(sentence, token)\n",
    "                    idf = compute_idf(token)\n",
    "                    tfidf_score += tf * idf\n",
    "            sentence_scores.append((i, tfidf_score))  # Add sentence index and score\n",
    "\n",
    "        # Sort by tf-idf score (descending) and return sentence indices\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        rank_list = [doc[0] for doc in sentence_scores]\n",
    "\n",
    "    return rank_list\n",
    "\n",
    "# Example Usage:\n",
    "sentences = [\n",
    "    [\"this\", \"is\", \"a\", \"sample\"],\n",
    "    [\"this\", \"is\", \"another\", \"example\"],\n",
    "    [\"sample\", \"document\", \"is\", \"sample\"]\n",
    "]\n",
    "query = \"sample example\"\n",
    "print(get_ranked_documents(sentences, \"inverted\", query))  # Ranking using inverted index\n",
    "print(get_ranked_documents(sentences, \"tfidf\", query))    # Ranking using tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_tfidf_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "    output =[[1.254,0,0,0.564,1.11],[2.12,1.254,0.564,0,0]]\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "npl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
